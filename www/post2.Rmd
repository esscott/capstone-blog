---
title: "Post 2"
---

Last August, on my birthday actually, I queued up *The Daily*^[https://www.nytimes.com/2020/08/03/podcasts/the-daily/algorithmic-justice-racism.html] podcast for my drive to work.  It was a special episode with guest producer Annie Brown. She interviewed Robert Julian-Borchak Williams, a black man who had been misidentified and arrested for shoplifting.  The facial recognition software employed by his precinct fed off of the racial bias of its creators and the selected training data.  In the process of his dramatic arrest, Williams questioned the officers as his wife and children watched from the front stoop: “This is not me. You think all Black men look alike?”  Referencing the super zoomed in image of the suspect, the officers agreed that it did not, in fact, look like Williams, but assured him that this was new standard procedure.  They trusted the technology more than they trusted their own intuition.  As an interviewee on *The Daily*, Williams acknowledged how lucky he was to have alibi in his family and colleagues, so that now he could share his story and raise awareness.  After spending time in jail for a crime that he did not commit, he was released.  The police department apologized profusely to him and his family — but the damage was done.  This wasn’t a one time occurence nor was it a mistake that law enforcement has taken seriously and learned from.

As described in *Data Feminism*, there’s a pattern here.  These algorithms are only as accurate as the data they’re trained on.  So, with mostly white male creators, racial bias is an overwhelmingly common ingredient.  For example, recently at MIT, Joy Buolamwini, a Ghanaian-American graduate student exposed a similar flaw in an all-together different facial recognition algorithm.  The software did not recognize her face at all, but when she drew a smiley face on her hand, it picked that up.  It had no problem identifying her classmates who had lighter skin.  Like Williams, Buolamwini gained publicity through the *New York Times* and other major news sources.  But, this raises another issue.  More often than not, those who are being disproportionately affected by these racist technologies are also the people who have to bear the burden of bringing these injustices, biases, and acts of systemic oppression to light.  As highlighted in *Data Feminism*, those who are coding and masterminding these ignorant algorithms need to take responsibility and make change without making more work for those who are most impacted.